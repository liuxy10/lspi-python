{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lspi.basis_functions import ExactBasis, RadialBasisFunction\n",
    "from lspi.policy import Policy\n",
    "from lspi.policy_ct import QuadraticPolicy\n",
    "from lspi.sample import Sample\n",
    "from lspi.solvers import LSTDQSolver\n",
    "import lspi\n",
    "from lspi import domains\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 12)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ssar = np.load(\"ssar.npy\")\n",
    "ssar.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from lspi.policy import Policy\n",
    "\n",
    "def lspi_loop_offline(solver, samples, discount, epsilon, max_iterations = 5, initial_policy=None):\n",
    "\n",
    "    # Initialize random seed\n",
    "    # np.random.seed(int(sum(100 * np.random.rand())))\n",
    "    # Create a new policy\n",
    "    policy = QuadraticPolicy(n_action= 1, n_state= 4, discount = discount)\n",
    "    if initial_policy is None:\n",
    "        initial_policy = policy\n",
    "\n",
    "    # Initialize policy iteration\n",
    "    iteration = 0\n",
    "    distance = float('inf')\n",
    "    all_policies = [initial_policy]\n",
    "\n",
    "    # If no samples, return\n",
    "    if not samples:\n",
    "        print('Warning: Empty sample set')\n",
    "        return policy, all_policies\n",
    "\n",
    "    # Main LSPI loop\n",
    "    while iteration < max_iterations and distance > epsilon:\n",
    "        # Update and print the number of iterations\n",
    "        iteration += 1\n",
    "        print('*********************************************************')\n",
    "        print(f'LSPI iteration: {iteration}')\n",
    "        iteration == 1\n",
    "\n",
    "        # Evaluate the current policy (and implicitly improve)\n",
    "        policy = lspi.learn(samples, initial_policy, solver)\n",
    "        # Compute the distance between the current and the previous policy\n",
    "        if len(policy.weights) == len(all_policies[-1].weights):\n",
    "            difference = policy.weights - all_policies[-1].weights\n",
    "            lmax_norm = np.linalg.norm(difference, np.inf)\n",
    "            l2_norm = np.linalg.norm(difference)\n",
    "        else:\n",
    "            lmax_norm = abs(np.linalg.norm(policy.weights, np.inf) -\n",
    "                            np.linalg.norm(all_policies[-1].weights, np.inf))\n",
    "            l2_norm = abs(np.linalg.norm(policy.weights) -\n",
    "                          np.linalg.norm(all_policies[-1].weights))\n",
    "        distance = l2_norm\n",
    "\n",
    "        # Print some information\n",
    "        print(f'   Norms -> Lmax: {lmax_norm:.6f}   L2: {l2_norm:.6f}')\n",
    "\n",
    "        # Store the current policy\n",
    "        all_policies.append(policy)\n",
    "\n",
    "    # Display some info\n",
    "    print('*********************************************************')\n",
    "    if distance > epsilon:\n",
    "        print(f'LSPI finished in {iteration} iterations WITHOUT CONVERGENCE to a fixed point')\n",
    "    else:\n",
    "        print(f'LSPI converged in {iteration} iterations')\n",
    "    print('*********************************************************')\n",
    "\n",
    "    return policy, all_policies\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_samples_from_file(filename):\n",
    "    samples = []\n",
    "    with open(filename, 'r') as file:\n",
    "        for line in file:\n",
    "            data = line.strip().split(',')\n",
    "            state = np.array([float(x) for x in data[0:4]])\n",
    "            action = int(data[4])\n",
    "            reward = float(data[5])\n",
    "            next_state = np.array([float(x) for x in data[6:10]])\n",
    "            done = bool(int(data[10]))\n",
    "            samples.append(Sample(state, action, reward, next_state, done))\n",
    "    return samples\n",
    "\n",
    "def generate_file(file_name, num_samples=1000):\n",
    "    with open(file_name, 'w') as file:\n",
    "        for _ in range(num_samples):\n",
    "            state = np.random.rand(4)\n",
    "            action = np.random.randint(0, 2)\n",
    "            reward = np.random.rand()\n",
    "            next_state = np.random.rand(4)\n",
    "            done = np.random.choice([0, 1])\n",
    "            file.write(f\"{','.join(map(str, state))},{action},{reward},{','.join(map(str, next_state))},{done}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*********************************************************\n",
      "LSPI iteration: 1\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (15,1) (5,1) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m solver \u001b[38;5;241m=\u001b[39m LSTDQSolver()\n\u001b[1;32m      6\u001b[0m samples \u001b[38;5;241m=\u001b[39m load_samples_from_file(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msamples.txt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 7\u001b[0m policy, all_policies \u001b[38;5;241m=\u001b[39m \u001b[43mlspi_loop_offline\u001b[49m\u001b[43m(\u001b[49m\u001b[43msolver\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdiscount\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.9\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_iterations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[2], line 32\u001b[0m, in \u001b[0;36mlspi_loop_offline\u001b[0;34m(solver, samples, discount, epsilon, max_iterations, initial_policy)\u001b[0m\n\u001b[1;32m     29\u001b[0m iteration \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# Evaluate the current policy (and implicitly improve)\u001b[39;00m\n\u001b[0;32m---> 32\u001b[0m policy \u001b[38;5;241m=\u001b[39m \u001b[43mlspi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial_policy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msolver\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# Compute the distance between the current and the previous policy\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(policy\u001b[38;5;241m.\u001b[39mweights) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(all_policies[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mweights):\n",
      "File \u001b[0;32m~/Documents/GitHub/lspi/lspi-python/lspi/lspi.py:26\u001b[0m, in \u001b[0;36mlearn\u001b[0;34m(data, initial_policy, solver, epsilon, max_iterations)\u001b[0m\n\u001b[1;32m     23\u001b[0m iteration \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# print(f\"data, current_policy, solver: {data}, {curr_policy}, {solver}\")\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m new_weights \u001b[38;5;241m=\u001b[39m \u001b[43msolver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msolve\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcurr_policy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m distance \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mnorm(new_weights \u001b[38;5;241m-\u001b[39m curr_policy\u001b[38;5;241m.\u001b[39mweights)\n\u001b[1;32m     28\u001b[0m curr_policy\u001b[38;5;241m.\u001b[39mweights \u001b[38;5;241m=\u001b[39m new_weights\n",
      "File \u001b[0;32m~/Documents/GitHub/lspi/lspi-python/lspi/solvers.py:96\u001b[0m, in \u001b[0;36mLSTDQSolver.solve\u001b[0;34m(self, data, policy)\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     94\u001b[0m         phi_sprime \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((k, \u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m---> 96\u001b[0m     a_mat \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m phi_sa\u001b[38;5;241m.\u001b[39mdot((\u001b[43mphi_sa\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mpolicy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdiscount\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mphi_sprime\u001b[49m)\u001b[38;5;241m.\u001b[39mT) \n\u001b[1;32m     97\u001b[0m     b_vec \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m phi_sa\u001b[38;5;241m*\u001b[39msample\u001b[38;5;241m.\u001b[39mreward \n\u001b[1;32m     98\u001b[0m \u001b[38;5;66;03m# print(f\"a_mat: {a_mat}\")\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (15,1) (5,1) "
     ]
    }
   ],
   "source": [
    "generate_file('samples.txt', num_samples=1000)\n",
    "# Load samples from the file\n",
    "samples = load_samples_from_file('samples.txt')\n",
    "# samples\n",
    "solver = LSTDQSolver()\n",
    "samples = load_samples_from_file('samples.txt')\n",
    "policy, all_policies = lspi_loop_offline(solver, samples, discount=0.9, epsilon=0.01, max_iterations=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sampling_policy = lspi.Policy(lspi.basis_functions.DummyBasis(2), .9, 1)\n",
    "\n",
    "samples = []\n",
    "for i in range(1000):\n",
    "    \n",
    "    action = sampling_policy.select_action(domain.current_state())\n",
    "    # print(f\"current state:{domain.current_state()} action:{action}\")\n",
    "    samples.append(domain.apply_action(action))\n",
    "    # print\n",
    "\n",
    "random_policy_cum_rewards = np.sum([sample.reward\n",
    "                                            for sample in samples])\n",
    "\n",
    "# samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(879, np.int64(207))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "solver = lspi.solvers.LSTDQSolver()\n",
    "\n",
    "\n",
    "initial_policy = lspi.Policy(\n",
    "    lspi.basis_functions.RadialBasisFunction(\n",
    "        means = np.array([[0], [2], [4], [6], [8]]), \n",
    "        gamma=.5, \n",
    "        num_actions=2),\n",
    "    discount=.9,\n",
    "    explore=0)\n",
    "\n",
    "learned_policy = lspi.learn(samples, initial_policy, solver)\n",
    "\n",
    "domain.reset()\n",
    "cumulative_reward = 0\n",
    "for i in range(1000):\n",
    "    # print(f\"current state:{domain.current_state()} action:{action}\")\n",
    "    action = learned_policy.select_action(domain.current_state())\n",
    "    sample = domain.apply_action(action)\n",
    "    cumulative_reward += sample.reward\n",
    "\n",
    "cumulative_reward, random_policy_cum_rewards\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
